{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression & variable selection\n",
    "\n",
    "- Ordinary least squares (OLS) method\n",
    "  - residual sum of squares (RSS)\n",
    "    - expectation: mean squared error (MSE)\n",
    "    - inference: $\\frac{(RSS_1 - RSS_0)/k}{RSS/(n-d-1)} \\sim F_{k,n-d-1} $ under null hypothesis $H_0: \\beta_1 = 0$\n",
    "    - nested model selection\n",
    "      - $F=\\frac{(RSS_0 - RSS_1)/(d_1-d_0)}{RSS_1/(n-d_1-1)}$\n",
    "      - F-statistic measures the change in RSS per additional parameter in the bigger model\n",
    "      - under the assumption that the smaller model is correct, $F \\sim F_{d_1-d_0, n-d_1-1}$\n",
    "  \n",
    "\n",
    "  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear regression & curse of dimensionality\n",
    "assume the linear model $Y=X^T\\beta+\\epsilon$ is true, with $\\epsilon \\sim N(0, \\sigma^2_{\\epsilon})$. \n",
    "- OLS estimator $\\hat{\\beta}=(X^TX)^{-1}X^TY$ is BLUE (best linear unbiased estimator)\n",
    "  - unbiased: $E[\\hat{\\beta}]=\\beta$\n",
    "  - the variance of $x_0^T\\hat{\\beta}$ increases linearly with d\n",
    "  - the prediction error is $\\sim \\frac{d}{n}\\sigma^2_{\\epsilon}+\\sigma^2_{\\epsilon}$\n",
    "    - the growth is slow only if n is large, and/or $\\sigma^2_{\\epsilon}$ is small\n",
    "\n",
    "how to avoid the curse of dimensionality?\n",
    "- dimension reduction; variable selection: choose a small subset of variables with strong effects\n",
    "- regularization: trade a little bias with large reduction in variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variale selection \n",
    "variable selection is a process of selecting a subset of the variables, fitting the selected model, and making inferences\n",
    "- include variables which are most predictive to the response\n",
    "- exclude noisy/uninformative variables\n",
    "\n",
    "how to perform variable selection?\n",
    "- best subset selection\n",
    "  - exhaustive search\n",
    "  - guarantee to find the best combination\n",
    "  - computation is infeasible for large $d \\ge 40$\n",
    "- sequential selection\n",
    "  - forward selection: add variables that produce large value of F statistic\n",
    "  - backward elimination: drop variables that produce small value of F statistic\n",
    "    - only applicable when $n > d$\n",
    "  - stepwise selection: in each step, consider both forward and backward moves and choose the best move\n",
    "    - allows previously added/dropped variables to be reconsidered\n",
    "    - greedy-search type; no theoretical guarantee to find the best model\n",
    "    - the selected models are highly variable\n",
    "- shrinkage methods: a systematic way controlling model complexity\n",
    "  - LASSO (Least Absolute Shrinkage and Selection Operator)\n",
    "  - Ridge regression\n",
    "  - Elastic net\n",
    "\n",
    "how to select the best model? for each candidate model, compute its AIC or BIC value. select the model that has the smallest AIC or BIC value.\n",
    "\n",
    "- AIC $= n\\log (RSS/n)+ 2 \\cdot df$\n",
    "- BIC $= n\\log (RSS/n)+ \\log(n) \\cdot df$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shrinkage methods\n",
    "a regularization method solves the following optimization problem:\n",
    "$$\\hat{\\beta}=\\underset{\\beta}{\\text{argmin}}\\left\\{ L(\\beta;y,X) + \\lambda J(\\beta) \\right\\}$$\n",
    "\n",
    "where L is the loss function, J is the shrinkage penalty function, and $\\lambda$ is the hyperparameter.\n",
    "\n",
    "- loss function\n",
    "  - For OLS: $L(\\beta;y,X)=\\sum_{i=1}^{n}[y_i-\\beta_0-\\sum_{j=1}^{d}x_{ij}\\beta_j]^2$\n",
    "  - For MLE methods, L=log likelihood\n",
    "  - For Cox's PH models, L is the partial likelihood\n",
    "  - In supervised learning, L is the hinge loss function (SVM), or exponential loss (AdaBoost)\n",
    "\n",
    "- Types of shrinkage penalties\n",
    "  - $J_0(|\\beta|)=\\sum_{j=1}^{d}I(\\beta_j \\ne 0)$\n",
    "  - $J_1(|\\beta|)=\\sum_{j=1}^{d}|\\beta_j|$ (LASSO)\n",
    "  - $J_2(|\\beta|)=\\sum_{j=1}^{d}\\beta_j^2$ (Ridge) \n",
    "$$\n",
    "J_q(|\\beta|)=\\lambda ||\\beta||_q^q=\\sum_{j=1}^{d}|\\beta_j|^q, q \\ge 0\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "- Ridge regression\n",
    "  - applying squared penalty on the coefficients $\\underset{\\beta}{\\text{argmin}}\\left\\{ \\sum_{i=1}^{n}(y_i-\\sum_{j=1}^{d}x_{ij}\\beta_j)^2 + \\lambda \\sum_{j=1}^d |\\beta_j|^2 \\right\\}$\n",
    "  - $\\hat{\\beta}^{ridge}=(X^TX+\\lambda I)^{-1}X^TY$\n",
    "  - $\\lambda \\ge 0$ is a hyper-parameter\n",
    "  - Ridge regression is a linear method that introduces bias but reduces the variance of the estimate\n",
    "  - $\\lambda \\rightarrow 0, \\hat{\\beta}^{ridge} \\rightarrow \\hat{\\beta}^{ols}$\n",
    "  - $\\lambda \\rightarrow \\infty, \\hat{\\beta}^{ridge} \\rightarrow 0$\n",
    "  - invertibility of $(X^TX+\\lambda I)$ is guaranteed, and thus a unique solution for $\\hat{\\beta}^{ridge}$\n",
    "- LASSO\n",
    "  - $\\hat{\\beta}=\\underset{\\beta}{\\text{argmin}}\\left\\{ \\frac{1}{2n}RSS + \\lambda \\sum_{j=1}^d |\\beta_j| \\right\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### variable selection beyond LASSO\n",
    "\n",
    "- SCAD\n",
    "- Adaptive lasso\n",
    "  - $\\hat{\\beta}^{alasso}=\\underset{\\beta}{\\text{argmin}}\\left\\{ ||y-X\\beta||^2 + \\lambda \\sum_{j=1}^{p} w_j|\\beta_j| \\right\\}$\n",
    "  - adaptive weights: adaptively chosen based on data\n",
    "  - large coefficients receive small weights (small penalties)\n",
    "  - small coefficients receive large weights (large penalties)\n",
    "  - how to compute the weights?\n",
    "    - intial estimates of parameters: $\\tilde{\\beta}_j=\\hat{\\beta}^{ols}$ or $\\hat{\\beta}^{ridgr}$ if collinearity is a concern\n",
    "    - $w_j = \\frac{1}{|\\tilde{\\beta}_j|^{\\gamma}}$\n",
    "- Elastic Net\n",
    "  - penalties for correlated predictors\n",
    "  - $\\hat{\\beta}^{enet}=\\underset{\\beta}{\\text{argmin}}\\left\\{||y-X\\beta||^2 + \\lambda_2||\\beta||^2 + \\lambda_1 ||\\beta|| \\right\\}$\n",
    "  - the estimator is called elatisc net\n",
    "  - ideally, eliminate trivial signals\n",
    "  - automatically include whole groups of signals into the model once one signal from that group is selected\n",
    "  - elastic net often outperforms the lasso regarding prediction accuracy\n",
    "- Adaptive Elastic Net\n",
    "  - $\\hat{\\beta}^{aenet}=\\underset{\\beta}{\\text{argmin}}\\left\\{||y-X\\beta||^2 + \\lambda_2||\\beta||^2 + \\lambda_1 \\sum_{j=1}^{p} w_j|\\beta_j| \\right\\}$\n",
    "  - has the oracle property; deals with the collinearity problem better\n",
    "- Group Lasso\n",
    "  - $\\hat{\\beta}^{glasso}=\\underset{\\beta}{\\text{argmin}}\\left\\{||y-X\\beta||^2 + \\lambda \\sum_{j=1}^{p} \\sqrt{p_j} ||\\beta_j|| \\right\\}$\n",
    "    - $||\\beta_j||=\\sqrt{\\beta_j^T\\beta_j}$ is the empirical L2 norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
